import os
import numpy as np
from paddleocr import PaddleOCR
from PIL import Image, ImageOps
import json
import re
import Levenshtein

# PaddleOCR ì´ˆê¸°í™”
ocr = PaddleOCR(lang='korean', use_angle_cls=True, use_gpu=True)

# ë°ì´í„° ê²½ë¡œ ì„¤ì •
IMAGE_PATH = r"E:\OCR\image"
GROUND_TRUTH_PATH = r"E:\OCR\correct"

# OCR ì •ë‹µì—ì„œ "xxx" ì œê±°
def clean_ground_truth(text):
    return ' '.join([word for word in text.split() if word != "xxx"]).strip()

# ì •ë‹µ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ë¡œë“œ
def load_ground_truth(json_path):
    try:
        with open(json_path, 'r', encoding='utf-8') as file:
            data = json.load(file)
            annotations = data.get('annotations', [])
            texts = [annotation.get('text', '') for annotation in annotations]
            raw_text = ' '.join(texts).strip()
            return clean_ground_truth(raw_text)
    except Exception as e:
        print(f"[âŒ ì—ëŸ¬] ì •ë‹µ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ - {json_path}: {str(e)}")
        return None

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°)
def clean_text(text):
    return re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', text).strip().lower()

# ì´ë¯¸ì§€ì—ì„œ ê°€ì¥ í° ë¬¸ì ì˜ì—­ ì¶”ì¶œ (ROI ì„ íƒ)
def extract_largest_text_region(image_path):
    try:
        with Image.open(image_path) as img:
            img = img.convert("L")  # í‘ë°± ë³€í™˜
            img = ImageOps.invert(img)  # ìƒ‰ìƒ ë°˜ì „ (í…ìŠ¤íŠ¸ ê°•ì¡°)
            img_array = np.array(img)

            # ì´ì§„í™”
            threshold = 128
            binary_img = np.where(img_array > threshold, 255, 0).astype(np.uint8)

            # ìœ¤ê³½ì„  ê²€ì¶œ
            non_zero_coords = np.column_stack(np.where(binary_img > 0))
            if non_zero_coords.shape[0] == 0:
                print("[âš ï¸ ê²½ê³ ] í…ìŠ¤íŠ¸ ì˜ì—­ì„ ì°¾ì§€ ëª»í•¨")
                return img  # ì›ë³¸ ì´ë¯¸ì§€ ë°˜í™˜

            # ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
            x_min, y_min = non_zero_coords.min(axis=0)
            x_max, y_max = non_zero_coords.max(axis=0)

            # ROI ì¶”ì¶œ (ë¬¸ì ì˜ì—­ë§Œ ìë¥´ê¸°)
            cropped_img = img.crop((y_min, x_min, y_max, x_max))
            return cropped_img

    except Exception as e:
        print(f"[âŒ ì˜¤ë¥˜] ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {image_path} - {str(e)}")
        return None

# í‰ê°€ í•¨ìˆ˜ (OCR ì„±ëŠ¥ ì¸¡ì •)
def token_based_accuracy(pred_text, gt_text):
    pred_tokens = set(pred_text.split())
    gt_tokens = set(gt_text.split())
    correct = len(pred_tokens & gt_tokens)
    total = len(gt_tokens)
    return correct, total

def substring_matching_accuracy(pred_text, gt_text):
    correct = sum(1 for token in gt_text.split() if token in pred_text)
    total = len(gt_text.split())
    return correct, total

def edit_distance_accuracy(pred_text, gt_text):
    distance = Levenshtein.distance(pred_text, gt_text)
    max_len = max(len(pred_text), len(gt_text))
    return max_len - distance, max_len

# í‰ê°€ ì‹¤í–‰
def evaluate_paddle_ocr(image_path, ground_truth_path):
    processed_images = 0
    skipped_files = []
    total_token_correct = 0
    total_token_total = 0
    total_substring_correct = 0
    total_substring_total = 0
    total_edit_distance_score = 0
    total_edit_distance_total = 0

    SUPPORTED_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif')

    if not os.path.exists(image_path) or len(os.listdir(image_path)) == 0:
        print("âŒ ì²˜ë¦¬í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return

    for file in os.listdir(image_path):
        if file.lower().endswith(SUPPORTED_EXTENSIONS):
            try:
                processed_images += 1
                print(f"\nğŸ“Œ í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ íŒŒì¼: {file} (ì´ {processed_images}ê°œ ì´ë¯¸ì§€ ì²˜ë¦¬ë¨)")

                image_file_path = os.path.join(image_path, file)
                json_file_path = os.path.join(ground_truth_path, os.path.splitext(file)[0] + '.json')

                if os.path.exists(json_file_path):
                    ground_truth = load_ground_truth(json_file_path)
                    if ground_truth is None:
                        skipped_files.append(file)
                        continue
                else:
                    print(f"âš ï¸ ì •ë‹µ íŒŒì¼ ì—†ìŒ: {json_file_path}")
                    continue

                # ê°€ì¥ í° ë¬¸ì ì˜ì—­ ì¶”ì¶œ
                cropped_image = extract_largest_text_region(image_file_path)
                if cropped_image is None:
                    skipped_files.append(file)
                    continue

                # OCR ì¸ì‹
                result = ocr.ocr(np.array(cropped_image), cls=True)

                # OCR ê²°ê³¼ í™•ì¸
                if not result or not result[0]:
                    print(f"âš ï¸ OCR ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŒ - íŒŒì¼: {file}")
                    continue

                paddle_text = ' '.join([line[1][0] for line in result[0]]).strip()
                print(f"ğŸ” OCR ê²°ê³¼: {paddle_text}")
                print(f"âœ… OCR ì •ë‹µ: {ground_truth}")

                # ì •í™•ë„ í‰ê°€
                token_correct, token_total = token_based_accuracy(paddle_text, ground_truth)
                substring_correct, substring_total = substring_matching_accuracy(paddle_text, ground_truth)
                edit_distance_score, edit_distance_total = edit_distance_accuracy(paddle_text, ground_truth)

                # ê°œë³„ ì´ë¯¸ì§€ ì •í™•ë„ ì¶œë ¥
                if token_total > 0 or substring_total > 0 or edit_distance_total > 0:
                    token_accuracy = (token_correct / token_total) * 100 if token_total > 0 else 0
                    substring_accuracy = (substring_correct / substring_total) * 100 if substring_total > 0 else 0
                    edit_distance_accuracy_result = (edit_distance_score / edit_distance_total) * 100 if edit_distance_total > 0 else 0

                    print(f"ğŸ“Š {file} ì´ë¯¸ì§€ ì •í™•ë„")
                    print(f"   ğŸ”¹ í† í° ë‹¨ìœ„ ì •í™•ë„: {token_accuracy:.2f}%")
                    print(f"   ğŸ”¹ ë¶€ë¶„ ë¬¸ìì—´ ì •í™•ë„: {substring_accuracy:.2f}%")
                    print(f"   ğŸ”¹ í¸ì§‘ ê±°ë¦¬ ê¸°ë°˜ ì •í™•ë„: {edit_distance_accuracy_result:.2f}%")

                # ê²°ê³¼ ëˆ„ì 
                total_token_correct += token_correct
                total_token_total += token_total
                total_substring_correct += substring_correct
                total_substring_total += substring_total
                total_edit_distance_score += edit_distance_score
                total_edit_distance_total += edit_distance_total

            except Exception as e:
                print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file} - {str(e)}")
                skipped_files.append(file)

    # ìµœì¢… í‰ê·  ì •í™•ë„ ì¶œë ¥
    final_token_accuracy = (total_token_correct / total_token_total) * 100 if total_token_total > 0 else 0
    final_substring_accuracy = (total_substring_correct / total_substring_total) * 100 if total_substring_total > 0 else 0
    final_edit_distance_accuracy = (total_edit_distance_score / total_edit_distance_total) * 100 if total_edit_distance_total > 0 else 0

    print("\nğŸ“Š ìµœì¢… í‰ê·  ì •í™•ë„ (ì „ì²´ ì´ë¯¸ì§€ í‰ê°€)")
    print(f"   ğŸ”¹ í† í° ë‹¨ìœ„ í‰ê·  ì •í™•ë„: {final_token_accuracy:.2f}%")
    print(f"   ğŸ”¹ ë¶€ë¶„ ë¬¸ìì—´ í‰ê·  ì •í™•ë„: {final_substring_accuracy:.2f}%")
    print(f"   ğŸ”¹ í¸ì§‘ ê±°ë¦¬ ê¸°ë°˜ í‰ê·  ì •í™•ë„: {final_edit_distance_accuracy:.2f}%")

# í‰ê°€ ì‹¤í–‰
evaluate_paddle_ocr(IMAGE_PATH, GROUND_TRUTH_PATH)
